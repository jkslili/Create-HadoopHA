1、测试flume
启动Hadoop、spark、zookeeper、hbase

在/opt/flume目录下创建flume-file-hdfs.conf文件
touch flume-file-hdfs.conf

添加如下内容：
# Name the components on this agent
a2.sources = r2
a2.sinks = k2
a2.channels = c2
# Describe/configure the source
a2.sources.r2.type = exec
#读取的文件地址,这儿我设置的是hive的日志目录
a2.sources.r2.command = tail -F /tmp/root/hive.log
#脚本的启动目录
a2.sources.r2.shell = /bin/bash -c
# Describe the sink
a2.sinks.k2.type = hdfs
#被读取文件的上传目录
a2.sinks.k2.hdfs.path = hdfs://master:9000/flume/%Y%m%d/%H
#上传文件的前缀
a2.sinks.k2.hdfs.filePrefix = logs-
#是否按照时间滚动文件夹
a2.sinks.k2.hdfs.round = true
#多少时间单位创建一个新的文件夹
a2.sinks.k2.hdfs.roundValue = 1
#重新定义时间单位
a2.sinks.k2.hdfs.roundUnit = hour
#是否使用本地时间戳
a2.sinks.k2.hdfs.useLocalTimeStamp = true
#积攒多少个Event才flush到HDFS一次
a2.sinks.k2.hdfs.batchSize = 1000
#设置文件类型，可支持压缩
a2.sinks.k2.hdfs.fileType = DataStream
#多久生成一个新的文件
a2.sinks.k2.hdfs.rollInterval = 600
#设置每个文件的滚动大小
a2.sinks.k2.hdfs.rollSize = 134217700
#文件的滚动与Event数量无关
a2.sinks.k2.hdfs.rollCount = 0
#最小冗余数
a2.sinks.k2.hdfs.minBlockReplicas = 1
# Use a channel which buffers events in memory
a2.channels.c2.type = memory
a2.channels.c2.capacity = 1000
a2.channels.c2.transactionCapacity = 100
# Bind the source and sink to the channel
a2.sources.r2.channels = c2
a2.sinks.k2.channel = c2

/opt/flume/flume1.9/bin/flume-ng agent --conf /opt/flume/flume1.9/conf/ --name a2 --conf-file /opt/flume/flume-file-hdfs.conf 

中途报错
错误: 找不到或无法加载主类 org.apache.flume.tools.GetJavaProperty或者Error: Could not find or load main class org.apache.flume.tools.GetJavaProperty

### 度娘说法以下：
    一般来说是由于装了HBASE等工具的原因
    [root@master conf]# flume-ng version
    Error: Could not find or load main class org.apache.flume.tools.GetJavaProperty

解决方法：

将Hbase的配置文件hbas-env.sh修改为：
1、将hbase的hbase.env.sh的一行配置注释掉
# Extra Java CLASSPATH elements. Optional.
#export HBASE_CLASSPATH=/home/hadoop/hbase/conf
 
2、或者将HBASE_CLASSPATH改为JAVA_CLASSPATH,配置如下
# Extra Java CLASSPATH elements. Optional.
export JAVA_CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar

修改hbase以后，再次运行

在开一个服务器窗口，启动hive：

在web查看，看是否有文件储存（我测试时，修改报错以后运行，web没有看到flume生成的文件，但是也没新的报错，等我琢磨出来再更新）

修改：
我重新换了一个测试，可能是第一个写的测试文件参数与我实际的虚拟机不符合

将flume-file-hdfs.conf重写，内容如下：
# Name the components on this agent
a2.sources = r2
a2.sinks = k2
a2.channels = c2

# Describe/configure the source
a2.sources.r2.type = netcat
#读取的文件地址,这儿我设置的是hive的日志目录
a2.sources.r2.bind =localhost
#脚本的启动目录
a2.sources.r2.port=2222

# Describe the sink
a2.sinks.k2.type = logger

# Use a channel which buffers events in memory
a2.channels.c2.type = memory
a2.channels.c2.capacity = 1000
a2.channels.c2.transactionCapacity = 100

# Bind the source and sink to the channel
a2.sources.r2.channels = c2
a2.sinks.k2.channel = c2

执行命令
/opt/flume/flume1.9/bin/flume-ng agent --conf /opt/flume/flume1.9/conf/ --name a2 --conf-file /opt/flume/flume-file-hdfs.conf -Dflume.root.logger=INFO,console

打开新的一个终端，telnet localhost 2222
输入信息 hello world！！！
之前创建agent的终端会输出相应的“Hello world!”字符串

flume测试完成！！！

2、kafka测试

在master执行
./kafka-topics.sh --create --zookeeper master:2181,node1:2181,node2:2181 --replication-factor 3 --partitions 3 --topic test  //创建topic-test

查看已创建的topic列表
./kafka-topics.sh --list --zookeeper localhost:2181

所有节点启动kafka
./kafka-server-start.sh -daemon ../config/server.properties &

master启动生产者
./kafka-console-producer.sh --broker-list master:9092,node1:9092,node2:9092 --topic test

其他节点启动消费者
./kafka-console-consumer.sh --bootstrap-server master:9092,node1:9092,node2:9092 --from-beginning --topic test

其他节点启动消费者可以看到生产者产生的数据

kafka测试完成！！！
